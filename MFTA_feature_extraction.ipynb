{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Installing required libraries for feature extraction\n",
    "!pip install numpy # For numerical operations and array manipulation\n",
    "!pip install pandas # Stores data from analysis into tables\n",
    "!pip install tqdm # Shows progress of analysis by showcasing a progress bar\n",
    "!pip install scipy # For signal and image preprocessing\n",
    "!pip install scikit-image # For image pre-processing\n",
    "!pip install opencv-python # Image resizing, conversion, and morphological operations\n",
    "!pip install tifffile # For manipulating .TIFF file images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e966e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Importing packages for feature extraction\n",
    "import os, glob, gzip, zlib, io # To interact with the operating system, searching for patterns in file, compression algorithm, and reserved import for errors in image compression or file saving\n",
    "\n",
    "import cv2 # For handling grayscale and multi-channel TIFFS\n",
    "import numpy as np # For numerical operations and array manipulation\n",
    "import pandas as pd # Stores data from analysis into tables\n",
    "import tifffile # For manipulating .TIFF file images\n",
    "from tqdm import tqdm # Shows progress of analysis by showcasing a progress bar\n",
    "\n",
    "\n",
    "from scipy.fft import fft2, fftshift # To apply Fast Fourier Transform operations\n",
    "from scipy.ndimage import sobel # Used to measure edge strength and local variations, and contribute to the pyramid gradient features\n",
    "from scipy.spatial import Delaunay, # build triangulations over image\n",
    "cKDTree # Used in correlation-based dimensions measure correlation integrals\n",
    "from scipy.signal import get_window # To apply window functions e.g., Rectangular, Barlett, Hamming etc.\n",
    "from skimage.exposure import histogram # For histogram computation \n",
    "from skimage.filters import threshold_otsu # To apply automatic otsu thresholding\n",
    "from skimage.measure import shannon_entropy # For measuring entropy\n",
    "from skimage.morphology import disk, diamond, square, dilation, erosion # Provide structuring elements (e.g., square, disk, diamond) for fractal/Minkowski measures\n",
    "from skimage.transform import resize # To create smaller versions of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Feature extraction code\n",
    "np.random.seed(0)\n",
    "\n",
    "# --------- CONFIG (EDIT YOUR PATHS) ----------\n",
    "GROUP_FOLDERS = {\n",
    "    \"Treatment_1_norm\": \"/User/Input/Treatment_1_norm\",\n",
    "    \"Treatment_2_norm\": \"/User/Input/Treatment_2_norm\",\n",
    "}\n",
    "OUTPUT_CSV = \"/Users/Output/MFTA_features.csv\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "\n",
    "# Downscale edge for heavy features (set lower for speed, higher for fidelity)\n",
    "MAX_SIDE_FOR_HEAVY = 1024  # e.g., 768 for faster runs, None to disable\n",
    "\n",
    "# --------- FIXED COLUMN ORDER (aligned to your segmented schema) ----------\n",
    "FEATURE_COLS = [\n",
    "    \"ParDim\",\"CubeCount\",\"TriangDim\",\"PoSpecDim\",\"SasDifBCDim\",\"DBC\",\"RDBC\",\n",
    "    \"CorDim\",\"Dir_Hor_Vert\",\"Dir_Mean_Rad\",\n",
    "    \"FFT_CA1_Rectangular\",\"FFT_MLS_Rectangular\",\"FFT_CA2_Rectangular\",\n",
    "    \"FFT_CA1_Bartlett\",\"FFT_MLS_Bartlett\",\"FFT_CA2_Bartlett\",\n",
    "    \"FFT_CA1_Hamming\",\"FFT_MLS_Hamming\",\"FFT_CA2_Hamming\",\n",
    "    \"FFT_CA1_Hanning\",\"FFT_MLS_Hanning\",\"FFT_CA2_Hanning\",\n",
    "    \"FFT_CA1_Blackman\",\"FFT_MLS_Blackman\",\"FFT_CA2_Blackman\",\n",
    "    \"FFT_CA1_Gaussian\",\"FFT_MLS_Gaussian\",\"FFT_CA2_Gaussian\",\n",
    "    \"FFT_CA1_Parzen\",\"FFT_MLS_Parzen\",\"FFT_CA2_Parzen\",\n",
    "    \"Hig1D_Single_cent\",\"Hig1D_Meander\",\"Hig1D_Mean_RC\",\"Hig1D_Mean_4RL\",\"Hig1D_Mean180RL\",\n",
    "    \"FAI_Single_cent\",\"FAI_Meander\",\"FAI_Mean_RC\",\"FAI_4RL\",\"FAI_180RL\",\n",
    "    \"Hig2D_KFD\",\"Hig2D_MultD_1\",\"Hig2D_Mult_D2\",\"Hig2D_Sq_D\",\"Hig2D_Direct_D\",\"Hig2D_Triangle\",\n",
    "    \"Mass_Rad_Dim\",\n",
    "    \"Mink_Blan_Sq\",\"Mink_Blan_Disk\",\"Mink_Blan_Diamond\",\"Mink_Blan_Hor\",\"Mink_Blan_Vert\",\n",
    "    \"Mink_VarDil_Sq\",\"Mink_VarDil_Disk\",\"Mink_VarDil_Diamond\",\"Mink_VarDil_Hor\",\"Mink_VarDil_Vert\",\n",
    "    \"RP_Lacunarity\",\"SV_Lacunarity\",\n",
    "    \"NKC_ZIP\",\"NKS_ZLIB\",\"NKS_GZIP\",\"RGB_KC_PNG\",\"RGB_KC_ZIP\",\n",
    "    \"FLBC_Differ\",\"FLBC_DVV\",\"FLBC_DVP1\",\"FLLBC_Differ\",\"FLLBC_DVV\",\"FLLBC_DVP1\",\n",
    "    \"FLSB_Differ\",\"FLSB_DVV\",\"FLSB_DVP1\",\"FLSBL_Differ\",\"FLSBL_DVV\",\"FLSBL_DVP1\",\n",
    "    \"MD_Dim_DVP1\",\"MD_Lac\",\n",
    "    \"GLCM_Contrast\",\"GLCM_Dissimilarity\",\"GLCM_Homogeneity\",\"GLCM_Energy\",\"GLCM_Correlation\",\n",
    "    \"GLCM_Entropy\",\"GLCM_Variance\",\"GLCM_Shade\",\"GLCM_Prominence\",\"GLCM_MaxProb\",\n",
    "    \"GLCM_Inertia\",\"GLCM_AngSM\",\"GLCM_IDM\",\n",
    "    \"Stat_Energy\",\"Stat_Entropy\",\n",
    "    \"PyrDim_DPDM_NN\",\"PyrDim_GPGM_NN\",\"PyrDim_Blan_NN\",\"PyrDim_Allo_NN\",\"PyrDim_ID_NN\",\"PyrDim_Kolm_NN\",\n",
    "    \"PyrDim_DPDM_Bilin\",\"PyrDim_GPGM_Bilin\",\"PyrDim_Blan_Bilin\",\"PyrDim_Allo_Bilin\",\"PyrDim_ID_Bilin\",\"PyrDim_Kolm_Bilin\",\n",
    "    \"PyrDim_DPDM_Bicub\",\"PyrDim_GPGM_Bicub\",\"PyrDim_Blan_Bicub\",\"PyrDim_Allo_Bicub\",\"PyrDim_ID_Bicub\",\"PyrDim_Kolm_Bicub\",\n",
    "    \"PyrDim_DPDM_Bicub2\",\"PyrDim_GPGM_Bicub2\",\"PyrDim_Blan_Bicub2\",\"PyrDim_Allo_Bicub2\",\"PyrDim_ID_Bicub2\",\"PyrDim_Kolm_Bicub2\",\n",
    "    \"PyrDim_DPDM_Aver\",\"PyrDim_GPGM_Aver\",\"PyrDim_Blan_Aver\",\"PyrDim_Allo_Aver\",\"PyrDim_ID_Aver\",\"PyrDim_Kolm_Aver\",\n",
    "    \"RGB_Hig_Diff_Kfold\",\"RGB_Hig_Diff_Mult\",\"RGB_Hig_Diff_SqD\",\"RGB_Hig_Kfolf\",\"RGB_Hig_Mult\",\"RGB_Hig_SqD\"\n",
    "]\n",
    "\n",
    "# --------- UTILS ----------\n",
    "def to_float01_gray_8bit(img):\n",
    "    arr = np.asarray(img)\n",
    "    if arr.ndim == 3:  # RGB/RGBA → gray\n",
    "        if arr.shape[0] in (3,4) and arr.shape[-1] not in (3,4):\n",
    "            arr = np.moveaxis(arr, 0, -1)\n",
    "        if arr.dtype != np.uint8:\n",
    "            arr = (arr.astype(np.float32) - arr.min()) / (arr.ptp() + 1e-12)\n",
    "            arr = (arr * 255).astype(np.uint8)\n",
    "        if arr.shape[-1] == 4:\n",
    "            arr = cv2.cvtColor(arr, cv2.COLOR_BGRA2GRAY)\n",
    "        else:\n",
    "            arr = cv2.cvtColor(arr, cv2.COLOR_BGR2GRAY)\n",
    "    elif arr.ndim != 2:\n",
    "        raise ValueError(f\"Unsupported array ndim={arr.ndim} for grayscale conversion.\")\n",
    "    if arr.dtype != np.uint8:\n",
    "        arr = (arr.astype(np.float32) - arr.min()) / (arr.ptp() + 1e-12)\n",
    "        arr = (arr * 255).astype(np.uint8)\n",
    "    return (arr.astype(np.float32) / 255.0).clip(0.0, 1.0)\n",
    "\n",
    "def downsample_mask(mask, shape):\n",
    "    return cv2.resize(mask.astype(np.uint8), (shape[1], shape[0]),\n",
    "                      interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "def safe_log(x):\n",
    "    return np.log(np.asarray(x, dtype=float) + 1e-12)\n",
    "\n",
    "def polyfit_safe(x, y):\n",
    "    if len(x) < 2 or len(y) < 2:\n",
    "        return 0.0\n",
    "    return float(np.polyfit(safe_log(np.asarray(x)), safe_log(np.asarray(y)), 1)[0])\n",
    "\n",
    "# --------- SUPER-ROBUST 2-D LOADER ----------\n",
    "def read_gray_2d(path):\n",
    "    \"\"\"\n",
    "    Always returns a true 2-D grayscale float array.\n",
    "    1) Try tifffile\n",
    "    2) Reduce N-D → 2-D (max-project any leading dims, handle channel orders)\n",
    "    3) If still not 2-D, fall back to cv2\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with tifffile.TiffFile(path) as tf:\n",
    "            arr = tf.asarray()\n",
    "        arr = np.asarray(arr)\n",
    "\n",
    "        # Reduce ≥4D: max-project all leading dims except the last two\n",
    "        if arr.ndim >= 4:\n",
    "            lead_axes = tuple(range(arr.ndim - 2))\n",
    "            arr = arr.max(axis=lead_axes)\n",
    "\n",
    "        if arr.ndim == 3:\n",
    "            # channels-last\n",
    "            if arr.shape[-1] in (3,4):\n",
    "                return to_float01_gray_8bit(arr)\n",
    "            # channels-first\n",
    "            if arr.shape[0] in (3,4):\n",
    "                arr = np.moveaxis(arr, 0, -1)\n",
    "                return to_float01_gray_8bit(arr)\n",
    "            # Z-stack → max-projection\n",
    "            arr = arr.max(axis=0)\n",
    "\n",
    "        if arr.ndim == 2:\n",
    "            return (arr.astype(np.float32) - arr.min()) / (arr.ptp() + 1e-12)\n",
    "\n",
    "        # 1-D or weird → fall back\n",
    "        raise ValueError(f\"tifffile produced ndim={arr.ndim}\")\n",
    "\n",
    "    except Exception as e_tiff:\n",
    "        # cv2 fallback\n",
    "        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"cv2 failed to read file: {e_tiff}\")\n",
    "        if img.ndim == 2:\n",
    "            arr = img.astype(np.float32)\n",
    "        elif img.ndim == 3:\n",
    "            if img.shape[-1] == 4:\n",
    "                arr = cv2.cvtColor(img, cv2.COLOR_BGRA2GRAY).astype(np.float32)\n",
    "            else:\n",
    "                arr = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "        else:\n",
    "            raise ValueError(f\"cv2 produced unsupported ndim={img.ndim}\")\n",
    "        return (arr - arr.min()) / (arr.ptp() + 1e-12)\n",
    "\n",
    "# --------- FEATURE PRIMITIVES (ROI-aware) ----------\n",
    "def partitioning_dimension(img, sizes=(4,8,16,32), mask=None):\n",
    "    img = np.asarray(img, float); H, W = img.shape\n",
    "    use_mask = mask is not None\n",
    "    if use_mask: mask = mask.astype(bool)\n",
    "    means, used = [], []\n",
    "    for s in sizes:\n",
    "        if s >= min(H, W): continue\n",
    "        local=[]\n",
    "        for i in range(0, H - s + 1, s):\n",
    "            for j in range(0, W - s + 1, s):\n",
    "                tile = img[i:i+s, j:j+s]\n",
    "                if use_mask:\n",
    "                    m = mask[i:i+s, j:j+s]\n",
    "                    if not np.any(m): continue\n",
    "                    v = tile[m]; \n",
    "                    if v.size == 0: continue\n",
    "                    local.append(float(np.var(v)))\n",
    "                else:\n",
    "                    if not np.any(tile): continue\n",
    "                    local.append(float(np.var(tile)))\n",
    "        if local:\n",
    "            means.append(np.mean(local)); used.append(s)\n",
    "    return 0.0 if len(used)<2 else polyfit_safe(1.0/np.asarray(used), np.asarray(means)+1e-12)\n",
    "\n",
    "def cube_counting(img, sizes=(2,4,8,16,32), mask=None):\n",
    "    img = np.asarray(img, float); H, W = img.shape\n",
    "    use_mask = mask is not None\n",
    "    if use_mask: mask = mask.astype(bool)\n",
    "    counts, invs = [], []\n",
    "    for s in sizes:\n",
    "        if s >= min(H, W): continue\n",
    "        gh, gw = H//s, W//s\n",
    "        if gh < 1 or gw < 1: continue\n",
    "        small = cv2.resize(img, (gw, gh), interpolation=cv2.INTER_LINEAR)\n",
    "        if use_mask:\n",
    "            m_small = downsample_mask(mask, small.shape)\n",
    "            n = int(np.count_nonzero((small > 0) & m_small))\n",
    "        else:\n",
    "            n = int(np.count_nonzero(small > 0))\n",
    "        counts.append(n); invs.append(1.0/s)\n",
    "    return 0.0 if len(counts)<2 else polyfit_safe(np.asarray(invs), np.asarray(counts)+1e-12)\n",
    "\n",
    "def triangulation_dimension(img, scales=(4,8,16,32,64), max_pts=60_000, mask=None):\n",
    "    I = np.asarray(img, float); H, W = I.shape\n",
    "    use_mask = mask is not None\n",
    "    if use_mask: M = mask.astype(bool)\n",
    "    areas, invs = [], []\n",
    "    for s in scales:\n",
    "        ny, nx = H//s, W//s\n",
    "        if ny < 3 or nx < 3 or ny*nx > max_pts: continue\n",
    "        y, x = np.mgrid[0:H:s, 0:W:s]\n",
    "        z = I[0:H:s, 0:W:s]\n",
    "        if use_mask:\n",
    "            m = M[0:H:s, 0:W:s]\n",
    "            if not np.any(m): continue\n",
    "            xs, ys, zs = x[m].astype(float), y[m].astype(float), z[m].astype(float)\n",
    "        else:\n",
    "            xs, ys, zs = x.ravel().astype(float), y.ravel().astype(float), z.ravel().astype(float)\n",
    "        if xs.size < 3: continue\n",
    "        try:\n",
    "            tri = Delaunay(np.column_stack((xs, ys)))\n",
    "        except Exception:\n",
    "            continue\n",
    "        P = np.column_stack((xs, ys, zs))\n",
    "        area = 0.0\n",
    "        for simplex in tri.simplices:\n",
    "            a, b, c = P[simplex]\n",
    "            area += 0.5 * np.linalg.norm(np.cross(b - a, c - a))\n",
    "        areas.append(max(area, 1e-12)); invs.append(1.0/s)\n",
    "    return 0.0 if len(areas)<2 else polyfit_safe(np.asarray(invs), np.asarray(areas))\n",
    "\n",
    "# --------- FFT (ROI-masked, auto-downscale for heavy) ----------\n",
    "def _radial_average(power2d):\n",
    "    h, w = power2d.shape\n",
    "    cy, cx = h//2, w//2\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    r = np.sqrt((y - cy)**2 + (x - cx)**2).astype(np.int32)\n",
    "    max_r = r.max()\n",
    "    radial_sum = np.bincount(r.ravel(), weights=power2d.ravel(), minlength=max_r+1)\n",
    "    radial_cnt = np.bincount(r.ravel(), minlength=max_r+1)\n",
    "    prof = radial_sum / np.maximum(radial_cnt, 1)\n",
    "    return prof[1:]  # drop DC\n",
    "\n",
    "def _window2d(name, shape, gaussian_std_frac=0.15):\n",
    "    h, w = shape\n",
    "    if name.lower() in (\"rectangular\",\"rect\"):\n",
    "        wy = np.ones(h, dtype=np.float32); wx = np.ones(w, dtype=np.float32)\n",
    "    elif name.lower() == \"gaussian\":\n",
    "        wy = get_window((\"gaussian\", max(1, int(h*gaussian_std_frac))), h, fftbins=False)\n",
    "        wx = get_window((\"gaussian\", max(1, int(w*gaussian_std_frac))), w, fftbins=False)\n",
    "    else:\n",
    "        mp = {\"hanning\":\"hann\",\"hann\":\"hann\",\"bartlett\":\"bartlett\",\"hamming\":\"hamming\",\"blackman\":\"blackman\",\"parzen\":\"parzen\"}\n",
    "        base = mp.get(name.lower(), name.lower())\n",
    "        wy = get_window(base, h, fftbins=False); wx = get_window(base, w, fftbins=False)\n",
    "    win2d = np.outer(wy, wx).astype(np.float32)\n",
    "    rms = np.sqrt(np.mean(win2d**2)) or 1.0\n",
    "    return win2d / rms\n",
    "\n",
    "def fft_features(img, mask=None):\n",
    "    img = np.asarray(img, float)\n",
    "    if mask is not None:\n",
    "        img = img * mask.astype(float)\n",
    "    windows = [\"Rectangular\",\"Bartlett\",\"Hamming\",\"Hanning\",\"Blackman\",\"Gaussian\",\"Parzen\"]\n",
    "    feats = {}\n",
    "    h, w = img.shape\n",
    "    img_use = img\n",
    "    if MAX_SIDE_FOR_HEAVY and max(h, w) > MAX_SIDE_FOR_HEAVY:\n",
    "        scale = MAX_SIDE_FOR_HEAVY / float(max(h, w))\n",
    "        img_use = cv2.resize(img, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)\n",
    "    for wname in windows:\n",
    "        win2d = _window2d(wname, img_use.shape)\n",
    "        iw = img_use * win2d\n",
    "        F = fftshift(fft2(iw))\n",
    "        power2d = (np.abs(F)**2).astype(np.float64) / img_use.size\n",
    "        prof = _radial_average(power2d)\n",
    "        k = np.arange(1, len(prof)+1, dtype=np.float64)\n",
    "        valid = (prof > 0)\n",
    "        if valid.sum() < 2:\n",
    "            slope, mlevel, integ = 0.0, float(prof.mean() if prof.size else 0.0), float(prof.sum())\n",
    "        else:\n",
    "            slope = float(np.polyfit(np.log(k[valid]), np.log(prof[valid]), 1)[0])\n",
    "            mlevel = float(np.mean(prof[valid]))\n",
    "            integ  = float(np.sum(prof[valid]))\n",
    "        feats[f\"FFT_CA1_{wname}\"] = slope\n",
    "        feats[f\"FFT_MLS_{wname}\"] = mlevel\n",
    "        feats[f\"FFT_CA2_{wname}\"] = integ\n",
    "    return feats\n",
    "\n",
    "# --------- TILE UTILITY ----------\n",
    "def _tile_vals(tile, m):\n",
    "    if m is not None:\n",
    "        if not np.any(m): return None\n",
    "        v = tile[m]\n",
    "        return v if v.size else None\n",
    "    return tile if np.any(tile) else None\n",
    "\n",
    "# --------- DBC FAMILY (ROI-aware) ----------\n",
    "def differential_box_counting_quantized(img01, sizes=(4,8,16,32), L=256, use_sum=True, mask=None):\n",
    "    img = np.clip(img01, 0, 1); H, W = img.shape\n",
    "    use_mask = mask is not None\n",
    "    if use_mask: mask = mask.astype(bool)\n",
    "    Y, X = [], []\n",
    "    for s in sizes:\n",
    "        if s >= min(H, W): continue\n",
    "        h = max(1, int(np.ceil(L / s)))\n",
    "        vals = []\n",
    "        for i in range(0, H - s + 1, s):\n",
    "            for j in range(0, W - s + 1, s):\n",
    "                tile = img[i:i+s, j:j+s]\n",
    "                m = mask[i:i+s, j:j+s] if use_mask else None\n",
    "                v = _tile_vals(tile, m)\n",
    "                if v is None: continue\n",
    "                Imin = int(np.floor(v.min() * (L-1)))\n",
    "                Imax = int(np.ceil (v.max() * (L-1)))\n",
    "                vals.append((Imax - Imin) // h + 1)\n",
    "        if not vals: continue\n",
    "        Y.append(np.sum(vals) if use_sum else np.mean(vals)); X.append(1.0 / s)\n",
    "    return 0.0 if len(Y)<2 else polyfit_safe(np.asarray(X), np.asarray(Y)+1e-12)\n",
    "\n",
    "def relative_differential_box_counting(img, sizes=(4,8,16,32), use_sum=True, mask=None):\n",
    "    H, W = img.shape\n",
    "    use_mask = mask is not None\n",
    "    if use_mask: mask = mask.astype(bool)\n",
    "    Y, X = [], []\n",
    "    for s in sizes:\n",
    "        if s >= min(H, W): continue\n",
    "        vals = []\n",
    "        for i in range(0, H - s + 1, s):\n",
    "            for j in range(0, W - s + 1, s):\n",
    "                tile = img[i:i+s, j:j+s]\n",
    "                m = mask[i:i+s, j:j+s] if use_mask else None\n",
    "                v = _tile_vals(tile, m)\n",
    "                if v is None: continue\n",
    "                Imin = float(v.min()); Imax = float(v.max())\n",
    "                vals.append((Imax - Imin) / (Imax + Imin + 1e-12))\n",
    "        if not vals: continue\n",
    "        Y.append(np.sum(vals) if use_sum else np.mean(vals)); X.append(1.0 / s)\n",
    "    return 0.0 if len(Y)<2 else polyfit_safe(np.asarray(X), np.asarray(Y)+1e-12)\n",
    "\n",
    "def sasaki_dbc_dimension(img, sizes=(4,8,16,32), alpha=0.5, use_sum=True, mask=None):\n",
    "    H, W = img.shape\n",
    "    gx = sobel(img, axis=1); gy = sobel(img, axis=0)\n",
    "    grad = np.hypot(gx, gy)\n",
    "    gnorm = np.clip(grad / (np.percentile(grad, 99.0) + 1e-12), 0.0, 1.0)\n",
    "    use_mask = mask is not None\n",
    "    if use_mask: mask = mask.astype(bool)\n",
    "    Y, X = [], []\n",
    "    for s in sizes:\n",
    "        if s >= min(H, W): continue\n",
    "        vals = []\n",
    "        for i in range(0, H - s + 1, s):\n",
    "            for j in range(0, W - s + 1, s):\n",
    "                tile = img[i:i+s, j:j+s]\n",
    "                m = mask[i:i+s, j:j+s] if use_mask else None\n",
    "                v = _tile_vals(tile, m)\n",
    "                if v is None: continue\n",
    "                dI = float(v.max() - v.min())\n",
    "                gmean = float(gnorm[i:i+s, j:j+s][m].mean() if use_mask else gnorm[i:i+s, j:j+s].mean())\n",
    "                vals.append(dI + alpha * gmean)\n",
    "        if not vals: continue\n",
    "        Y.append(np.sum(vals) if use_sum else np.mean(vals)); X.append(1.0 / s)\n",
    "    return 0.0 if len(Y)<2 else polyfit_safe(np.asarray(X), np.asarray(Y)+1e-12)\n",
    "\n",
    "# --------- CORRELATION DIMENSIONS ----------\n",
    "def correlation_dimension_gp(img, radii=None, max_points=5000):\n",
    "    coords = np.argwhere(img > 0)\n",
    "    N = len(coords)\n",
    "    if N < 3: return 0.0\n",
    "    if N > max_points:\n",
    "        sel = np.random.choice(N, size=max_points, replace=False)\n",
    "        coords = coords[sel]; N = len(coords)\n",
    "    if radii is None:\n",
    "        rmax = max(2.0, 0.25 * min(img.shape))\n",
    "        radii = np.geomspace(1.0, rmax, 10)\n",
    "    tree = cKDTree(coords.astype(np.float64))\n",
    "    Cr = []\n",
    "    for r in radii:\n",
    "        counts = tree.query_ball_point(coords, r, return_length=True) - 1\n",
    "        Cr.append(counts.sum() / (N * (N - 1) + 1e-12))\n",
    "    y = np.array(Cr, float); k = np.array(radii, float)\n",
    "    valid = (y > 0)\n",
    "    if valid.sum() < 2: return 0.0\n",
    "    return float(np.polyfit(np.log(k[valid]), np.log(y[valid]), 1)[0])\n",
    "\n",
    "def _pairwise_within_r(coords, rmax):\n",
    "    tree = cKDTree(coords)\n",
    "    pairs = list(tree.query_pairs(rmax))\n",
    "    if len(pairs) == 0:\n",
    "        return np.zeros((0,2), dtype=int)\n",
    "    return np.array(pairs, dtype=int)\n",
    "\n",
    "def directional_correlation_dimension(img, mode=\"hv\", radii=None, angle_tol_deg=10, max_points=5000):\n",
    "    coords = np.argwhere(img > 0)\n",
    "    N = len(coords)\n",
    "    if N < 3: return 0.0\n",
    "    if N > max_points:\n",
    "        sel = np.random.choice(N, size=max_points, replace=False)\n",
    "        coords = coords[sel]; N = len(coords)\n",
    "    if radii is None:\n",
    "        rmax = max(2.0, 0.25 * min(img.shape))\n",
    "        radii = np.geomspace(1.0, rmax, 10)\n",
    "    rmax = radii.max()\n",
    "    pairs = _pairwise_within_r(coords.astype(np.float64), rmax)\n",
    "    if pairs.shape[0] == 0:\n",
    "        return 0.0\n",
    "    d = coords[pairs[:,1]] - coords[pairs[:,0]]\n",
    "    dist = np.hypot(d[:,0], d[:,1]) + 1e-12\n",
    "    ang  = (np.degrees(np.arctan2(d[:,0], d[:,1])) + 360.0) % 180.0\n",
    "\n",
    "    def slope_for_wedge(target_deg):\n",
    "        tol = angle_tol_deg\n",
    "        m = (np.abs(((ang - target_deg + 90) % 180) - 90) <= tol)\n",
    "        if m.sum() < 5: return None\n",
    "        Cr = [np.mean(dist[m] <= r) for r in radii]\n",
    "        Cr = np.clip(np.array(Cr, float), 1e-12, 1.0)\n",
    "        ok = Cr > 0\n",
    "        if ok.sum() < 2: return None\n",
    "        return float(np.polyfit(np.log(radii[ok]), np.log(Cr[ok]), 1)[0])\n",
    "\n",
    "    if mode == \"hv\":\n",
    "        vals = [s for s in (slope_for_wedge(0.0), slope_for_wedge(90.0)) if s is not None]\n",
    "        return float(np.mean(vals)) if vals else 0.0\n",
    "    if mode == \"rad4\":\n",
    "        targets = [0.0, 45.0, 90.0, 135.0]\n",
    "        vals = [slope_for_wedge(t) for t in targets if slope_for_wedge(t) is not None]\n",
    "        return float(np.mean(vals)) if vals else 0.0\n",
    "    return correlation_dimension_gp(img, radii=radii, max_points=max_points)\n",
    "\n",
    "# --------- HIGUCHI 1D + FAI ----------\n",
    "def higuchi_1d_signal(sig, kmax=10):\n",
    "    sig = np.asarray(sig, dtype=float).ravel()\n",
    "    n = sig.size\n",
    "    if n < 3: return 0.0\n",
    "    kmax = int(max(2, min(kmax, n // 2)))\n",
    "    ks = np.arange(1, kmax + 1)\n",
    "    Lk = np.empty_like(ks, dtype=float)\n",
    "    for i, k in enumerate(ks):\n",
    "        segs = [np.abs(np.diff(sig[m::k])).mean() for m in range(k) if sig[m::k].size > 1]\n",
    "        Lk[i] = np.mean(segs) if segs else 0.0\n",
    "    mask = Lk > 0\n",
    "    if mask.sum() < 2: return 0.0\n",
    "    x = np.log(1.0 / ks[mask] + 1e-12); y = np.log(Lk[mask] + 1e-12)\n",
    "    return float(np.polyfit(x, y, 1)[0])\n",
    "\n",
    "def sample_line_nn(img, y0, x0, y1, x1, n=None):\n",
    "    if n is None:\n",
    "        n = int(np.hypot(y1 - y0, x1 - x0)) + 1\n",
    "    ys = np.clip(np.rint(np.linspace(y0, y1, n)), 0, img.shape[0]-1).astype(int)\n",
    "    xs = np.clip(np.rint(np.linspace(x0, x1, n)), 0, img.shape[1]-1).astype(int\n",
    "    )\n",
    "    return img[ys, xs]\n",
    "\n",
    "def radial_profiles(img, angles_deg):\n",
    "    h, w = img.shape\n",
    "    cy, cx = (h - 1) / 2.0, (w - 1) / 2.0\n",
    "    rmax = 0.5 * np.sqrt(h*h + w*w)\n",
    "    profiles = []\n",
    "    for a in angles_deg:\n",
    "        th = np.deg2rad(a)\n",
    "        dy, dx = np.sin(th), np.cos(th)\n",
    "        y0, x0 = cy - rmax * dy, cx - rmax * dx\n",
    "        y1, x1 = cy + rmax * dy, cx + rmax * dx\n",
    "        profiles.append(sample_line_nn(img, y0, x0, y1, x1))\n",
    "    return profiles\n",
    "\n",
    "def higuchi_1d(img, mode=\"meander\"):\n",
    "    \"\"\"\n",
    "    If given a 2-D image, build a 1-D profile per `mode` then compute Higuchi.\n",
    "    If given a 1-D array (e.g., a row/column profile), compute Higuchi directly.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(img)\n",
    "    # NEW: handle 1-D inputs robustly\n",
    "    if arr.ndim == 1:\n",
    "        return higuchi_1d_signal(arr)\n",
    "\n",
    "    rows = arr.mean(1)\n",
    "    cols = arr.mean(0)\n",
    "\n",
    "    if mode == \"single\":\n",
    "        prof = rows\n",
    "    elif mode == \"meander\":\n",
    "        prof = np.concatenate([rows, cols])\n",
    "    elif mode == \"meanrc\":\n",
    "        if len(rows) != len(cols):\n",
    "            cols_resampled = np.interp(np.linspace(0, len(cols) - 1, len(rows)),\n",
    "                                       np.arange(len(cols)), cols)\n",
    "        else:\n",
    "            cols_resampled = cols\n",
    "        prof = 0.5 * (rows + cols_resampled)\n",
    "    elif mode == \"4rl\":\n",
    "        prof = np.concatenate([arr[0, :], arr[-1, :], arr[:, 0], arr[:, -1]])\n",
    "    elif mode == \"180rl\":\n",
    "        rad180 = radial_profiles(arr, np.arange(180))\n",
    "        prof = np.concatenate([p for p in rad180 if p.size > 1]) if len(rad180) > 0 else rows\n",
    "    else:\n",
    "        prof = rows\n",
    "\n",
    "    return higuchi_1d_signal(prof)\n",
    "\n",
    "def fai(img):\n",
    "    return float(abs(higuchi_1d(img, \"single\") - higuchi_1d(img, \"meander\")))\n",
    "\n",
    "# --------- HIGUCHI 2D VARIANTS ----------\n",
    "def _higuchi2d_surface_measures(img, kmax=10, variant=\"kfold\"):\n",
    "    I = np.asarray(img, np.float64); H, W = I.shape\n",
    "    ks = np.arange(1, kmax + 1, dtype=int)\n",
    "    Lk = []\n",
    "    for k in ks:\n",
    "        if H <= k or W <= k: break\n",
    "        dx  = np.abs(I[:, k:]   - I[:, :-k])\n",
    "        dy  = np.abs(I[k:, :]   - I[:-k, :])\n",
    "        dxy = np.abs(I[k:, k:]  - I[:-k, :-k])\n",
    "        if variant == \"kfold\":\n",
    "            val = k * (dx.mean() + dy.mean() + dxy.mean())\n",
    "        elif variant == \"mult1\":\n",
    "            h = min(dx.shape[0], dy.shape[0]); w = min(dx.shape[1], dy.shape[1])\n",
    "            val = (dx[:h, :w] * dy[:h, :w]).mean()\n",
    "        elif variant == \"mult2\":\n",
    "            h = min(dxy.shape[0], dx.shape[0], dy.shape[0]); w = min(dxy.shape[1], dx.shape[1], dy.shape[1])\n",
    "            val = (dxy[:h, :w] * (0.5 * (dx[:h, :w] + dy[:h, :w]))).mean()\n",
    "        elif variant in (\"sqd\", \"squared\"):\n",
    "            val = (dx**2).mean() + (dy**2).mean() + (dxy**2).mean()\n",
    "        elif variant == \"direct\":\n",
    "            val = 0.5 * (dx.mean() + dy.mean())\n",
    "        elif variant == \"triangle\":\n",
    "            tri = np.abs(I[k:, k:] + I[:-k, :-k] - I[k:, :-k] - I[:-k, k:])\n",
    "            val = tri.mean()\n",
    "        else:\n",
    "            val = (dx**2).mean() + (dy**2).mean() + (dxy**2).mean()\n",
    "        Lk.append(val + 1e-12)\n",
    "    return ks[:len(Lk)], np.asarray(Lk, np.float64)\n",
    "\n",
    "def _higuchi2d_slope_from_Lk(ks, Lk):\n",
    "    if len(Lk) < 2: return 0.0\n",
    "    x = np.log(1.0 / ks.astype(np.float64)); y = np.log(Lk)\n",
    "    return float(np.polyfit(x, y, 1)[0])\n",
    "\n",
    "def higuchi_2d(img, kmax=10, variant=\"direct\"):\n",
    "    ks, Lk = _higuchi2d_surface_measures(img, kmax=kmax, variant=variant if variant!=\"sqd\" else \"squared\")\n",
    "    return _higuchi2d_slope_from_Lk(ks, Lk)\n",
    "\n",
    "# --------- LACUNARITY ----------\n",
    "def fraclac_lacunarity(img, box_sizes=(4,8,16,32), mode=\"raster\", mask=None):\n",
    "    img = np.asarray(img, float); H, W = img.shape\n",
    "    use_mask = mask is not None\n",
    "    if use_mask: mask = mask.astype(bool)\n",
    "    results = []\n",
    "    for s in box_sizes:\n",
    "        if s >= min(H, W): continue\n",
    "        masses = []\n",
    "        if mode == \"raster\":\n",
    "            for oy in range(s):\n",
    "                for ox in range(s):\n",
    "                    y_end = H - (H - oy) % s\n",
    "                    x_end = W - (W - ox) % s\n",
    "                    for y in range(oy, y_end, s):\n",
    "                        for x in range(ox, x_end, s):\n",
    "                            tile = img[y:y+s, x:x+s]\n",
    "                            m = mask[y:y+s, x:x+s] if use_mask else None\n",
    "                            v = _tile_vals(tile, m)\n",
    "                            if v is None: continue\n",
    "                            masses.append(v.sum())\n",
    "        else:\n",
    "            for y in range(0, H - s + 1):\n",
    "                for x in range(0, W - s + 1):\n",
    "                    tile = img[y:y+s, x:x+s]\n",
    "                    m = mask[y:y+s, x:x+s] if use_mask else None\n",
    "                    v = _tile_vals(tile, m)\n",
    "                    if v is None: continue\n",
    "                    masses.append(v.sum())\n",
    "        if len(masses) < 2: continue\n",
    "        masses = np.array(masses, float)\n",
    "        mu, var = masses.mean(), masses.var()\n",
    "        lam = var / (mu*mu + 1e-12)\n",
    "        results.append((1.0/s, lam))\n",
    "    if len(results) < 2: return 0.0\n",
    "    inv_s, lam_vals = zip(*results)\n",
    "    return float(np.polyfit(np.log(inv_s), np.log(lam_vals), 1)[0])\n",
    "\n",
    "def lacunarity_features(img, mask=None):\n",
    "    return {\n",
    "        \"RP_Lacunarity\": fraclac_lacunarity(img, mode=\"raster\",  mask=mask),\n",
    "        \"SV_Lacunarity\": fraclac_lacunarity(img, mode=\"sliding\", mask=mask),\n",
    "    }\n",
    "\n",
    "# --------- STRICT FRACLAC-STYLE BOX COUNTING ----------\n",
    "def _fraclac_binary(img01):\n",
    "    return (np.asarray(img01, float) > 0)\n",
    "\n",
    "def _boxes_raster(binary, s, oy, ox):\n",
    "    H, W = binary.shape\n",
    "    ys = range(oy, H - (H - oy) % s, s)\n",
    "    xs = range(ox, W - (W - ox) % s, s)\n",
    "    for y in ys:\n",
    "        for x in xs:\n",
    "            yield binary[y:y+s, x:x+s]\n",
    "\n",
    "def _boxes_sliding(binary, s):\n",
    "    H, W = binary.shape\n",
    "    for y in range(0, H - s + 1):\n",
    "        for x in range(0, W - s + 1):\n",
    "            yield binary[y:y+s, x:x+s]\n",
    "\n",
    "def _fraclac_metrics_for_mode(binary, sizes, mode):\n",
    "    invs = []; Nocc=[]; VarM=[]; MeanM=[]\n",
    "    for s in sizes:\n",
    "        if s < 2 or s > min(binary.shape): continue\n",
    "        masses = []\n",
    "        if mode == \"raster\":\n",
    "            for oy in range(s):\n",
    "                for ox in range(s):\n",
    "                    masses.extend(int(t.sum()) for t in _boxes_raster(binary, s, oy, ox))\n",
    "        elif mode == \"sliding\":\n",
    "            masses.extend(int(t.sum()) for t in _boxes_sliding(binary, s))\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'raster' or 'sliding'\")\n",
    "        masses = np.asarray(masses, float)\n",
    "        if masses.size == 0: continue\n",
    "        invs.append(1.0 / s)\n",
    "        Nocc.append(float(np.count_nonzero(masses > 0)))\n",
    "        VarM.append(float(masses.var() + 1e-12))\n",
    "        MeanM.append(float(masses.mean()))\n",
    "    if len(invs) < 2: return 0.0, 0.0, 0.0\n",
    "    invs = np.asarray(invs, float)\n",
    "    Nocc = np.asarray(Nocc, float)\n",
    "    VarM = np.asarray(VarM, float)\n",
    "    MeanM = np.asarray(MeanM, float)\n",
    "    differ = float(np.polyfit(np.log(invs), np.log(np.maximum(Nocc, 1e-12)), 1)[0]) if (Nocc > 0).sum() >= 2 else 0.0\n",
    "    dvv    = float(np.polyfit(np.log(invs), np.log(np.maximum(VarM, 1e-12)), 1)[0]) if (VarM > 0).sum() >= 2 else 0.0\n",
    "    dvp1   = float(np.polyfit(np.log(invs), np.log(MeanM + 1.0 + 1e-12), 1)[0]) if MeanM.size >= 2 else 0.0\n",
    "    return differ, dvv, dvp1\n",
    "\n",
    "def fraclac_strict(img01, sizes=(4,8,16,32,64)):\n",
    "    B = _fraclac_binary(img01)\n",
    "    differ_r, dvv_r, dvp1_r = _fraclac_metrics_for_mode(B, sizes, mode=\"raster\")\n",
    "    differ_s, dvv_s, dvp1_s = _fraclac_metrics_for_mode(B, sizes, mode=\"sliding\")\n",
    "\n",
    "    def slope(pairs):\n",
    "        if len(pairs) < 2: return 0.0\n",
    "        x = np.log([p[0] for p in pairs]); y = np.log([p[1] for p in pairs])\n",
    "        return float(np.polyfit(x, y, 1)[0])\n",
    "\n",
    "    # Local variants (medians across origins / positions)\n",
    "    def _local(binary, sizes, mode):\n",
    "        diffs, dvvs, dvp1s = [], [], []\n",
    "        if mode == \"raster\":\n",
    "            for s in sizes:\n",
    "                if s < 2 or s > min(binary.shape): continue\n",
    "                inv = 1.0 / s\n",
    "                nocc_list, var_list, mean_list = [], [], []\n",
    "                for oy in range(s):\n",
    "                    for ox in range(s):\n",
    "                        masses = [int(t.sum()) for t in _boxes_raster(binary, s, oy, ox)]\n",
    "                        if len(masses) == 0: continue\n",
    "                        masses = np.asarray(masses, float)\n",
    "                        nocc_list.append(float(np.count_nonzero(masses > 0)))\n",
    "                        var_list.append(float(masses.var() + 1e-12))\n",
    "                        mean_list.append(float(masses.mean()))\n",
    "                if not nocc_list: continue\n",
    "                diffs.append((inv, float(np.median(nocc_list))))\n",
    "                dvvs.append((inv, float(np.median(var_list))))\n",
    "                dvp1s.append((inv, float(np.median(np.asarray(mean_list) + 1.0))))\n",
    "        else:\n",
    "            for s in sizes:\n",
    "                if s < 2 or s > min(binary.shape): continue\n",
    "                inv = 1.0 / s\n",
    "                masses = [int(t.sum()) for t in _boxes_sliding(binary, s)]\n",
    "                if len(masses) == 0: continue\n",
    "                masses = np.asarray(masses, float)\n",
    "                diffs.append((inv, float(np.count_nonzero(masses > 0))))\n",
    "                dvvs.append((inv, float(np.median((masses - masses.mean())**2) + 1e-12)))\n",
    "                dvp1s.append((inv, float(masses.mean() + 1.0)))\n",
    "        return slope(diffs), slope(dvvs), slope(dvp1s)\n",
    "\n",
    "    differ_lr, dvv_lr, dvp1_lr = _local(B, sizes, \"raster\")\n",
    "    differ_ls, dvv_ls, dvp1_ls = _local(B, sizes, \"sliding\")\n",
    "\n",
    "    return {\n",
    "        \"FLBC_Differ\": differ_r, \"FLBC_DVV\": dvv_r, \"FLBC_DVP1\": dvp1_r,\n",
    "        \"FLLBC_Differ\": differ_lr, \"FLLBC_DVV\": dvv_lr, \"FLLBC_DVP1\": dvp1_lr,\n",
    "        \"FLSB_Differ\": differ_s, \"FLSB_DVV\": dvv_s, \"FLSB_DVP1\": dvp1_s,\n",
    "        \"FLSBL_Differ\": differ_ls, \"FLSBL_DVV\": dvv_ls, \"FLSBL_DVP1\": dvp1_ls,\n",
    "    }\n",
    "\n",
    "# --------- MASS vs DISTANCE (strict binary) ----------\n",
    "def md_dim_dvp1_binary(img, n_r=12, r_min=1.0, r_max_frac=0.45):\n",
    "    B = (np.asarray(img, float) > 0)\n",
    "    coords = np.argwhere(B)\n",
    "    if coords.shape[0] < 2: return 0.0\n",
    "    cy, cx = coords.mean(0)\n",
    "    yy, xx = np.mgrid[0:B.shape[0], 0:B.shape[1]]\n",
    "    dmap = np.hypot(yy - cy, xx - cx)\n",
    "    r_max = max(2.0, r_max_frac * min(B.shape))\n",
    "    rads = np.geomspace(max(1.0, r_min), r_max, n_r)\n",
    "    d_sorted = np.sort(dmap[B])\n",
    "    M = []\n",
    "    for r in rads:\n",
    "        k = np.searchsorted(d_sorted, r, side=\"right\")\n",
    "        M.append(float(k))\n",
    "    M = np.asarray(M, float)\n",
    "    if M.size < 2: return 0.0\n",
    "    return float(np.polyfit(np.log(rads), np.log(M + 1.0), 1)[0])\n",
    "\n",
    "def md_lacunarity_binary(img, n_r=12, r_min=3.0, r_max_frac=0.45):\n",
    "    B = (np.asarray(img, float) > 0)\n",
    "    coords = np.argwhere(B)\n",
    "    if coords.shape[0] < 2: return 0.0\n",
    "    cy, cx = coords.mean(0)\n",
    "    yy, xx = np.mgrid[0:B.shape[0], 0:B.shape[1]]\n",
    "    dmap = np.hypot(yy - cy, xx - cx)\n",
    "    r_max = max(4.0, r_max_frac * min(B.shape))\n",
    "    edges = np.geomspace(max(1.0, r_min), r_max, n_r+1)\n",
    "    shell_vals, shell_r = [], []\n",
    "    for i in range(n_r):\n",
    "        r0, r1 = edges[i], edges[i+1]\n",
    "        m = (dmap >= r0) & (dmap < r1)\n",
    "        vals = B[m].astype(float)\n",
    "        if vals.size < 2: continue\n",
    "        mu = float(vals.mean()); var = float(vals.var())\n",
    "        lam = var / (mu*mu + 1e-12)\n",
    "        shell_vals.append(lam); shell_r.append(np.sqrt(r0*r1))\n",
    "    if len(shell_vals) < 2: return 0.0\n",
    "    shell_vals = np.asarray(shell_vals, float); shell_r = np.asarray(shell_r, float)\n",
    "    return float(np.polyfit(np.log(shell_r), np.log(shell_vals + 1e-12), 1)[0])\n",
    "\n",
    "def mass_vs_distance_strict(img):\n",
    "    return {\"MD_Dim_DVP1\": md_dim_dvp1_binary(img), \"MD_Lac\": md_lacunarity_binary(img)}\n",
    "\n",
    "# --------- GLCM (ROI-masked; no external entropy()) ----------\n",
    "def _glcm_masked(img01, mask, levels=32, distances=(1,), angles=(0, np.pi/4, np.pi/2, 3*np.pi/4)):\n",
    "    imgq = np.clip((img01 * (levels - 1)).round(), 0, levels - 1).astype(np.uint8)\n",
    "    H, W = imgq.shape\n",
    "    M = mask.astype(bool)\n",
    "\n",
    "    def angle_to_offset(theta):\n",
    "        ct, st = np.cos(theta), np.sin(theta)\n",
    "        dx = int(np.round(ct)); dy = int(np.round(st))\n",
    "        return dy, dx\n",
    "\n",
    "    P_sum = np.zeros((levels, levels), dtype=np.float64)\n",
    "    any_pairs = False\n",
    "    for d in distances:\n",
    "        for theta in angles:\n",
    "            dy, dx = angle_to_offset(theta)\n",
    "            dy *= d; dx *= d\n",
    "            y0 = max(0, -dy); y1 = min(H, H - dy)\n",
    "            x0 = max(0, -dx); x1 = min(W, W - dx)\n",
    "            if y1 - y0 <= 0 or x1 - x0 <= 0: continue\n",
    "            A = imgq[y0:y1, x0:x1]; B = imgq[y0+dy:y1+dy, x0+dx:x1+dx]\n",
    "            M1 = M[y0:y1, x0:x1];   M2 = M[y0+dy:y1+dy, x0+dx:x1+dx]\n",
    "            valid = M1 & M2\n",
    "            if not np.any(valid): continue\n",
    "            any_pairs = True\n",
    "            a = A[valid].ravel(); b = B[valid].ravel()\n",
    "            P = np.zeros((levels, levels), dtype=np.float64)\n",
    "            np.add.at(P, (a, b), 1.0)\n",
    "            P_sum += P\n",
    "\n",
    "    if not any_pairs:\n",
    "        return P_sum  # all zeros\n",
    "    s = P_sum.sum()\n",
    "    if s <= 0:\n",
    "        return P_sum\n",
    "    return P_sum / s\n",
    "\n",
    "def glcm_features(img01, mask, levels=32, distances=(1,), angles=(0, np.pi/4, np.pi/2, 3*np.pi/4)):\n",
    "    P = _glcm_masked(img01, mask, levels=levels, distances=distances, angles=angles)\n",
    "    if P.sum() <= 0:\n",
    "        # Return zeros, avoids NameError/NaNs\n",
    "        return {k:0.0 for k in [\n",
    "            \"GLCM_AngSM\",\"GLCM_IDM\",\"GLCM_Contrast\",\"GLCM_Energy\",\"GLCM_Entropy\",\"GLCM_Homogeneity\",\n",
    "            \"GLCM_Variance\",\"GLCM_Shade\",\"GLCM_Prominence\",\"GLCM_Inertia\",\"GLCM_Correlation\",\n",
    "            \"GLCM_Dissimilarity\",\"GLCM_MaxProb\"\n",
    "        ]}\n",
    "    I, J = np.meshgrid(np.arange(levels), np.arange(levels), indexing=\"ij\")\n",
    "    px = P.sum(axis=1); py = P.sum(axis=0)\n",
    "    mux = (px * np.arange(levels)).sum()\n",
    "    muy = (py * np.arange(levels)).sum()\n",
    "    sigx = np.sqrt(((np.arange(levels) - mux) ** 2 * px).sum())\n",
    "    sigy = np.sqrt(((np.arange(levels) - muy) ** 2 * py).sum())\n",
    "\n",
    "    diff = I - J; sum_ij = I + J\n",
    "    asm   = (P**2).sum()\n",
    "    ent   = float(-np.sum(P * (np.log(P + 1e-12))))  # no external entropy()\n",
    "    idm   = float((P / (1.0 + diff**2)).sum())\n",
    "    contr = float(((diff**2) * P).sum())\n",
    "    var_x = ((np.arange(levels) - mux)**2 * px).sum()\n",
    "    var_y = ((np.arange(levels) - muy)**2 * py).sum()\n",
    "    var   = float(0.5 * (var_x + var_y))\n",
    "    shade = float((((sum_ij - (mux + muy))**3) * P).sum())\n",
    "    prom  = float((((sum_ij - (mux + muy))**4) * P).sum())\n",
    "    corr  = float((((I - mux) * (J - muy) * P).sum()) / (sigx * sigy)) if (sigx>0 and sigy>0) else 1.0\n",
    "    dissim   = float((np.abs(diff) * P).sum())\n",
    "    maxprob  = float(P.max())\n",
    "\n",
    "    return {\n",
    "        \"GLCM_AngSM\": float(asm),\n",
    "        \"GLCM_IDM\": float(idm),\n",
    "        \"GLCM_Contrast\": float(contr),\n",
    "        \"GLCM_Energy\": float(asm),\n",
    "        \"GLCM_Entropy\": float(ent),\n",
    "        \"GLCM_Homogeneity\": float(idm),\n",
    "        \"GLCM_Variance\": float(var),\n",
    "        \"GLCM_Shade\": float(shade),\n",
    "        \"GLCM_Prominence\": float(prom),\n",
    "        \"GLCM_Inertia\": float(contr),\n",
    "        \"GLCM_Correlation\": float(corr),\n",
    "        \"GLCM_Dissimilarity\": float(dissim),\n",
    "        \"GLCM_MaxProb\": float(maxprob),\n",
    "    }\n",
    "\n",
    "# --------- BASIC STATS ----------\n",
    "def stats(img01, mask=None):\n",
    "    if mask is not None:\n",
    "        v = img01[mask]\n",
    "        if v.size == 0: return {\"Stat_Energy\": 0.0, \"Stat_Entropy\": 0.0}\n",
    "        return {\"Stat_Energy\": float(np.sum(v**2)), \"Stat_Entropy\": float(shannon_entropy(v))}\n",
    "    return {\"Stat_Energy\": float(np.sum(img01**2)), \"Stat_Entropy\": float(shannon_entropy(img01))}\n",
    "\n",
    "# --------- PYRAMID (ROI-masked) ----------\n",
    "def _downsample_levels(img, order, max_levels=5):\n",
    "    levels = [img]\n",
    "    for _ in range(1, max_levels):\n",
    "        h, w = levels[-1].shape\n",
    "        if min(h, w) < 16: break\n",
    "        levels.append(resize(levels[-1], (h//2, w//2), order=order, anti_aliasing=True, preserve_range=True))\n",
    "    return levels\n",
    "\n",
    "def _downsample_masks(mask, shapes):\n",
    "    ms = [mask]\n",
    "    for i in range(1, len(shapes)):\n",
    "        ms.append(downsample_mask(ms[-1], shapes[i]))\n",
    "    return ms\n",
    "\n",
    "def _log_slope(x, y):\n",
    "    x = np.asarray(x, float); y = np.asarray(y, float)\n",
    "    m = (x > 0) & (y > 0)\n",
    "    if m.sum() < 2: return 0.0\n",
    "    return float(np.polyfit(np.log(x[m]), np.log(y[m]), 1)[0])\n",
    "\n",
    "def pyramid_features(img01, mask=None):\n",
    "    feats = {}\n",
    "    interps = {\"NN\": 0, \"Bilin\": 1, \"Bicub\": 3, \"Bicub2\": 3, \"Aver\": 1}\n",
    "    base = np.clip(img01, 0, 1)\n",
    "    M = mask.astype(bool) if mask is not None else (base > 0)\n",
    "    try:\n",
    "        thr = float(threshold_otsu(base[M])) if np.any(M) else 0.5\n",
    "    except Exception:\n",
    "        thr = 0.5\n",
    "\n",
    "    for name, order in interps.items():\n",
    "        levels = _downsample_levels(base, order)\n",
    "        shapes = [L.shape for L in levels]\n",
    "        masks  = _downsample_masks(M, shapes)\n",
    "        if len(levels) < 2:\n",
    "            for key in [f\"PyrDim_DPDM_{name}\", f\"PyrDim_GPGM_{name}\", f\"PyrDim_Blan_{name}\",\n",
    "                        f\"PyrDim_Allo_{name}\", f\"PyrDim_ID_{name}\", f\"PyrDim_Kolm_{name}\"]:\n",
    "                feats[key] = 0.0\n",
    "            continue\n",
    "\n",
    "        scales = [float(2**i) for i in range(len(levels))]\n",
    "\n",
    "        dpdm_vals = []\n",
    "        for L, Mm in zip(levels, masks):\n",
    "            dx = np.abs(np.diff(L, axis=1)); dy = np.abs(np.diff(L, axis=0))\n",
    "            Mx = Mm[:,1:] & Mm[:,:-1]; My = Mm[1:,:] & Mm[:-1,:]\n",
    "            v = (dx[Mx].mean() if np.any(Mx) else 0.0) + (dy[My].mean() if np.any(My) else 0.0)\n",
    "            dpdm_vals.append(v + 1e-12)\n",
    "        feats[f\"PyrDim_DPDM_{name}\"] = _log_slope(scales, dpdm_vals)\n",
    "\n",
    "        pgm_vals = []\n",
    "        for L, Mm in zip(levels, masks):\n",
    "            gx = sobel(L, axis=1); gy = sobel(L, axis=0)\n",
    "            g = np.hypot(gx, gy)\n",
    "            pgm_vals.append(g[Mm].mean() + 1e-12 if np.any(Mm) else 0.0)\n",
    "        feats[f\"PyrDim_GPGM_{name}\"] = _log_slope(scales, pgm_vals)\n",
    "\n",
    "        bln_vals = []\n",
    "        for L, Mm in zip(levels, masks):\n",
    "            if not np.any(Mm): bln_vals.append(0.0); continue\n",
    "            h, w = L.shape\n",
    "            r = max(1, int(round(0.01 * min(h, w))))\n",
    "            se = disk(r)\n",
    "            u8 = np.clip(L * 255, 0, 255).astype(np.uint8)\n",
    "            bl = (dilation(u8, se).astype(np.float32) - erosion(u8, se))\n",
    "            bln_vals.append(bl[Mm].mean() + 1e-12)\n",
    "        feats[f\"PyrDim_Blan_{name}\"] = _log_slope(scales, bln_vals)\n",
    "\n",
    "        allo_vals = []\n",
    "        for L, Mm in zip(levels, masks):\n",
    "            allo_vals.append(float((L[Mm] > thr).sum() + 1e-12))\n",
    "        feats[f\"PyrDim_Allo_{name}\"] = _log_slope(scales, allo_vals)\n",
    "\n",
    "        id_vals = []\n",
    "        for L, Mm in zip(levels, masks):\n",
    "            if not np.any(Mm): id_vals.append(0.0); continue\n",
    "            counts, _ = histogram(np.clip(L[Mm], 0, 1), nbins=32)\n",
    "            p = counts.astype(np.float64); p = p / (p.sum() + 1e-12)\n",
    "            id_vals.append(float(-(p * np.log(p + 1e-12)).sum()) + 1e-12)\n",
    "        feats[f\"PyrDim_ID_{name}\"] = _log_slope(scales, id_vals)\n",
    "\n",
    "        kolm_vals = []\n",
    "        for L, Mm in zip(levels, masks):\n",
    "            if not np.any(Mm): kolm_vals.append(0.0); continue\n",
    "            ys, xs = np.where(Mm)\n",
    "            y0,y1,x0,x1 = ys.min(), ys.max()+1, xs.min(), xs.max()+1\n",
    "            u8 = np.clip((L[y0:y1, x0:x1]) * 255, 0, 255).astype(np.uint8)\n",
    "            bytes_len = len(cv2.imencode(\".png\", u8)[1].tobytes())\n",
    "            kolm_vals.append(float(bytes_len) + 1e-12)\n",
    "        feats[f\"PyrDim_Kolm_{name}\"] = _log_slope(scales, kolm_vals)\n",
    "\n",
    "    return feats\n",
    "\n",
    "# --------- COMPLEXITY ----------\n",
    "def nkc(img32):\n",
    "    arr = np.asarray(img32, np.float32)\n",
    "    arr = (arr - np.min(arr)) / (np.ptp(arr) + 1e-12)\n",
    "    arr8 = np.rint(arr * 255).astype(np.uint8)\n",
    "    raw = arr8.tobytes()\n",
    "    uncompressed = max(len(raw), 1)\n",
    "    nkc_zlib = len(zlib.compress(raw)) / uncompressed\n",
    "    nkc_gzip = len(gzip.compress(raw)) / uncompressed\n",
    "    return {\"NKC_ZIP\": float(nkc_zlib), \"NKS_ZLIB\": float(nkc_zlib), \"NKS_GZIP\": float(nkc_gzip)}\n",
    "\n",
    "def rgb_kc(img01):\n",
    "    rgb = np.stack([img01, img01, img01], axis=-1)\n",
    "    rgbu8 = np.clip(rgb * 255, 0, 255).astype(np.uint8)\n",
    "    raw = rgbu8.tobytes()\n",
    "    uncompressed = max(len(raw), 1)\n",
    "    return {\n",
    "        \"RGB_KC_PNG\": len(cv2.imencode(\".png\", rgbu8)[1].tobytes()) / uncompressed,\n",
    "        \"RGB_KC_ZIP\": len(zlib.compress(raw)) / uncompressed,\n",
    "    }\n",
    "\n",
    "# --------- RGB HIGUCHI (grayscale tripled) ----------\n",
    "def rgb_higuchi(img01, kmax=10):\n",
    "    arr = np.asarray(img01, np.float64)\n",
    "    chans = [arr, arr, arr]\n",
    "    def slope_for_variant(variant):\n",
    "        vals = []\n",
    "        for c in chans:\n",
    "            ks, Lk = _higuchi2d_surface_measures(c, kmax=kmax, variant=variant)\n",
    "            vals.append(_higuchi2d_slope_from_Lk(ks, Lk) if len(ks)>=2 else 0.0)\n",
    "        return float(np.mean(vals)) if vals else 0.0\n",
    "    diff_kfold = slope_for_variant(\"kfold\")\n",
    "    diff_sqd   = slope_for_variant(\"sqd\")\n",
    "    diff_mult1 = slope_for_variant(\"mult1\")\n",
    "    diff_mult2 = slope_for_variant(\"mult2\")\n",
    "    diff_mult  = 0.5 * (diff_mult1 + diff_mult2)\n",
    "    return {\n",
    "        \"RGB_Hig_Diff_Kfold\": diff_kfold,\n",
    "        \"RGB_Hig_Diff_Mult\":  diff_mult,\n",
    "        \"RGB_Hig_Diff_SqD\":   diff_sqd,\n",
    "        \"RGB_Hig_Kfold\":      diff_kfold,\n",
    "        \"RGB_Hig_Mult\":       diff_mult,\n",
    "        \"RGB_Hig_SqD\":        diff_sqd,\n",
    "    }\n",
    "\n",
    "# --------- MASS RADIUS (intensity-weighted center) ----------\n",
    "def mass_radius(img, n_r=12, r_min=1.0, r_max_frac=0.45, intensity_weighted=True):\n",
    "    img = np.asarray(img, np.float64)\n",
    "    H, W = img.shape\n",
    "    if intensity_weighted and img.max() > 0:\n",
    "        yy, xx = np.mgrid[0:H, 0:W]; tot = img.sum()\n",
    "        cy = float((yy * img).sum() / (tot + 1e-12))\n",
    "        cx = float((xx * img).sum() / (tot + 1e-12))\n",
    "        field = img\n",
    "    else:\n",
    "        coords = np.argwhere(img > 0)\n",
    "        if len(coords) < 2: return 0.0\n",
    "        cy, cx = coords.mean(0); field = (img > 0).astype(np.float64)\n",
    "    yy, xx = np.mgrid[0:H, 0:W]\n",
    "    dmap = np.hypot(yy - cy, xx - cx)\n",
    "    r_max = max(2.0, r_max_frac * min(H, W))\n",
    "    rads = np.geomspace(max(1.0, r_min), r_max, n_r)\n",
    "    flat_d = dmap.ravel(); flat_m = field.ravel()\n",
    "    order = np.argsort(flat_d)\n",
    "    d_sorted = flat_d[order]; m_sorted = flat_m[order]\n",
    "    csum = np.cumsum(m_sorted)\n",
    "    M = []\n",
    "    for r in rads:\n",
    "        kidx = np.searchsorted(d_sorted, r, side=\"right\") - 1\n",
    "        M.append(0.0 if kidx < 0 else float(csum[kidx]))\n",
    "    M = np.asarray(M) + 1e-12\n",
    "    return float(np.polyfit(np.log(rads), np.log(M), 1)[0])\n",
    "\n",
    "# --------- MINKOWSKI ----------\n",
    "def minkowski_dimension(img, radii=(1,2,4,8,16), shape=\"disk\"):\n",
    "    I = np.asarray(img, float)\n",
    "    I = (I - I.min()) / (I.max() - I.min() + 1e-12)\n",
    "    Iu8 = np.clip(I * 255, 0, 255).astype(np.uint8)\n",
    "    vols = []\n",
    "    for r in radii:\n",
    "        if shape == \"square\": selem = square(2*r + 1).astype(np.uint8)\n",
    "        elif shape == \"diamond\": selem = diamond(r).astype(np.uint8)\n",
    "        else: selem = disk(r).astype(np.uint8)\n",
    "        dil = cv2.dilate(Iu8, selem); ero = cv2.erode(Iu8, selem)\n",
    "        vols.append(float((dil.astype(np.float32) - ero.astype(np.float32)).sum()) + 1e-12)\n",
    "    if len(vols) < 2: return 0.0\n",
    "    return float(np.polyfit(np.log(radii), np.log(vols), 1)[0])\n",
    "\n",
    "def minkowski_variants(img):\n",
    "    def minkowski_dim_oriented(I, radii, orient):\n",
    "        I = np.asarray(I, float)\n",
    "        I = (I - I.min()) / (I.max() - I.min() + 1e-12)\n",
    "        Iu8 = np.clip(I * 255, 0, 255).astype(np.uint8)\n",
    "        vols = []\n",
    "        for r in radii:\n",
    "            if orient == \"hor\":\n",
    "                selem = np.ones((1, 2*r+1), np.uint8)\n",
    "            elif orient == \"vert\":\n",
    "                selem = np.ones((2*r+1, 1), np.uint8)\n",
    "            else:\n",
    "                selem = disk(r).astype(np.uint8)\n",
    "            dil = cv2.dilate(Iu8, selem); ero = cv2.erode(Iu8, selem)\n",
    "            vols.append(float((dil.astype(np.float32) - ero.astype(np.float32)).sum()) + 1e-12)\n",
    "        if len(vols) < 2: return 0.0\n",
    "        return float(np.polyfit(np.log(radii), np.log(vols), 1)[0])\n",
    "\n",
    "    radii=(1,2,4,8,16)\n",
    "    return {\n",
    "        \"Mink_Blan_Sq\":   minkowski_dimension(img, shape=\"square\"),\n",
    "        \"Mink_Blan_Disk\": minkowski_dimension(img, shape=\"disk\"),\n",
    "        \"Mink_Blan_Diamond\": minkowski_dimension(img, shape=\"diamond\"),\n",
    "        \"Mink_Blan_Hor\":  minkowski_dim_oriented(img, radii, \"hor\"),\n",
    "        \"Mink_Blan_Vert\": minkowski_dim_oriented(img, radii, \"vert\"),\n",
    "        \"Mink_VarDil_Sq\":   minkowski_dimension(img, shape=\"square\"),\n",
    "        \"Mink_VarDil_Disk\": minkowski_dimension(img, shape=\"disk\"),\n",
    "        \"Mink_VarDil_Diamond\": minkowski_dimension(img, shape=\"diamond\"),\n",
    "        \"Mink_VarDil_Hor\":  minkowski_dim_oriented(img, radii, \"hor\"),\n",
    "        \"Mink_VarDil_Vert\": minkowski_dim_oriented(img, radii, \"vert\"),\n",
    "    }\n",
    "\n",
    "# --------- ONE-IMAGE ANALYSIS ----------\n",
    "def analyze_one(path, group):\n",
    "    raw2d = read_gray_2d(path)                    # robust → 2-D gray [0,1]\n",
    "    img01 = np.clip(raw2d, 0, 1).astype(np.float32)\n",
    "    roi = img01 > 0                               # ROI = pixels > 0\n",
    "    if not np.any(roi):\n",
    "        raise ValueError(\"Image has empty ROI (no pixels > 0).\")\n",
    "    base = img01 * roi.astype(img01.dtype)\n",
    "\n",
    "    # Optional: downscale for heavy ops (keeps ROI)\n",
    "    if MAX_SIDE_FOR_HEAVY and max(base.shape) > MAX_SIDE_FOR_HEAVY:\n",
    "        scale = MAX_SIDE_FOR_HEAVY / float(max(base.shape))\n",
    "        base = cv2.resize(base, (int(base.shape[1]*scale), int(base.shape[0]*scale)), interpolation=cv2.INTER_AREA)\n",
    "        roi  = base > 0\n",
    "\n",
    "    feats = {}\n",
    "    # Core dims & spectra\n",
    "    feats[\"ParDim\"]    = partitioning_dimension(base, mask=roi)\n",
    "    feats[\"CubeCount\"] = cube_counting(base, mask=roi)\n",
    "    feats[\"TriangDim\"] = triangulation_dimension(base, mask=roi)\n",
    "    feats.update(fft_features(base, mask=roi))\n",
    "    feats[\"PoSpecDim\"] = feats.get(\"FFT_CA1_Rectangular\", np.nan)\n",
    "\n",
    "    # DBC family\n",
    "    feats[\"SasDifBCDim\"] = sasaki_dbc_dimension(base, sizes=(4,8,16,32), alpha=0.5, mask=roi)\n",
    "    feats[\"DBC\"]  = differential_box_counting_quantized(base, sizes=(4,8,16,32), L=256, mask=roi)\n",
    "    feats[\"RDBC\"] = relative_differential_box_counting(base, sizes=(4,8,16,32), mask=roi)\n",
    "\n",
    "    # Correlation dims\n",
    "    feats[\"CorDim\"]       = correlation_dimension_gp(base, radii=None, max_points=5000)\n",
    "    feats[\"Dir_Hor_Vert\"] = directional_correlation_dimension(base, mode=\"hv\",   radii=None, angle_tol_deg=10, max_points=5000)\n",
    "    feats[\"Dir_Mean_Rad\"] = directional_correlation_dimension(base, mode=\"rad4\", radii=None, angle_tol_deg=10, max_points=5000)\n",
    "\n",
    "    # Higuchi 1D profiles & FAI\n",
    "    H, W = base.shape\n",
    "    center_row = base[H//2, :]; center_col = base[:, W//2]\n",
    "    feats[\"Hig1D_Single_cent\"] = higuchi_1d(base, \"single\")\n",
    "    feats[\"Hig1D_Meander\"]     = higuchi_1d(base, \"meander\")\n",
    "    feats[\"Hig1D_Mean_RC\"]     = 0.5 * (higuchi_1d(center_row) + higuchi_1d(center_col))\n",
    "    feats[\"Hig1D_Mean_4RL\"]    = higuchi_1d(base, \"4rl\")\n",
    "    feats[\"Hig1D_Mean180RL\"]   = higuchi_1d(base, \"180rl\")\n",
    "    feats[\"FAI_Single_cent\"]   = float(abs(higuchi_1d(center_row) - higuchi_1d(center_col)))\n",
    "    feats[\"FAI_Meander\"]       = float(abs(higuchi_1d(base, \"meander\") - higuchi_1d(base, \"single\")))\n",
    "    feats[\"FAI_Mean_RC\"]       = float(abs(higuchi_1d(center_row) - higuchi_1d(center_col)))\n",
    "    feats[\"FAI_4RL\"]           = float(np.std([higuchi_1d(p) for p in radial_profiles(base, [0,45,90,135])]))\n",
    "    feats[\"FAI_180RL\"]         = float(np.std([higuchi_1d(p) for p in radial_profiles(base, np.arange(180))]))\n",
    "\n",
    "    # Higuchi 2D variants\n",
    "    feats[\"Hig2D_KFD\"]        = _higuchi2d_slope_from_Lk(*_higuchi2d_surface_measures(base, kmax=10, variant=\"kfold\"))\n",
    "    feats[\"Hig2D_MultD_1\"]    = _higuchi2d_slope_from_Lk(*_higuchi2d_surface_measures(base, kmax=10, variant=\"mult1\"))\n",
    "    feats[\"Hig2D_Mult_D2\"]    = _higuchi2d_slope_from_Lk(*_higuchi2d_surface_measures(base, kmax=10, variant=\"mult2\"))\n",
    "    feats[\"Hig2D_Sq_D\"]       = higuchi_2d(base, variant=\"squared\")\n",
    "    feats[\"Hig2D_Direct_D\"]   = higuchi_2d(base, variant=\"direct\")\n",
    "    feats[\"Hig2D_Triangle\"]   = higuchi_2d(base, variant=\"triangle\")\n",
    "\n",
    "    # Mass-radius, Minkowski, Lacunarity\n",
    "    feats[\"Mass_Rad_Dim\"] = mass_radius(base, intensity_weighted=True)\n",
    "    feats.update(minkowski_variants(base))\n",
    "    lac = lacunarity_features(base, mask=roi)\n",
    "    feats[\"RP_Lacunarity\"] = lac[\"RP_Lacunarity\"]\n",
    "    feats[\"SV_Lacunarity\"] = lac[\"SV_Lacunarity\"]\n",
    "\n",
    "    # Complexity\n",
    "    feats.update(nkc(base))\n",
    "    feats.update(rgb_kc(base))\n",
    "    feats.update(fraclac_strict(base))\n",
    "    feats.update(mass_vs_distance_strict(base))\n",
    "\n",
    "    # ROI-masked GLCM / Stats / Pyramid / RGB-Higuchi\n",
    "    feats.update(glcm_features(base, mask=roi, levels=32))\n",
    "    feats.update(stats(base, mask=roi))\n",
    "    feats.update(pyramid_features(base, mask=roi))\n",
    "    feats.update(rgb_higuchi(base))\n",
    "\n",
    "    # Ensure all requested columns exist\n",
    "    for k in FEATURE_COLS:\n",
    "        feats.setdefault(k, np.nan)\n",
    "\n",
    "    # Back-compat typo\n",
    "    if \"RGB_Hig_Kfold\" in feats and \"RGB_Hig_Kfolf\" not in feats:\n",
    "        feats[\"RGB_Hig_Kfolf\"] = feats[\"RGB_Hig_Kfold\"]\n",
    "\n",
    "    feats[\"File\"] = os.path.basename(path)\n",
    "    feats[\"Group\"] = group\n",
    "    return feats\n",
    "\n",
    "# --------- BATCH ----------\n",
    "def batch_analyze(group_folders, output_csv):\n",
    "    records=[]\n",
    "    for group, folder in group_folders.items():\n",
    "        files = sorted([f for f in glob.glob(os.path.join(folder, \"*.tif\"))] +\n",
    "                       [f for f in glob.glob(os.path.join(folder, \"*.tiff\"))])\n",
    "        print(f\"\\n Found {len(files)} TIFF images in {group}: {folder}\")\n",
    "        for fpath in tqdm(files, desc=f\"Processing {group}\"):\n",
    "            try:\n",
    "                rec = analyze_one(fpath, group)\n",
    "                records.append(rec)\n",
    "            except Exception as e:\n",
    "                print(f\" Skipped {os.path.basename(fpath)}: {e}\")\n",
    "\n",
    "    if not records:\n",
    "        print(\"No records produced. Check your inputs.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    cols = [\"File\",\"Group\"] + FEATURE_COLS\n",
    "    extras = [c for c in df.columns if c not in cols]\n",
    "    df = df[[c for c in cols if c in df.columns] + extras]\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\n Saved → {output_csv}\")\n",
    "    return df\n",
    "\n",
    "# --------- RUN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    batch_analyze(GROUP_FOLDERS, OUTPUT_CSV)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
